{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# st.cache\n",
    "\n",
    "st.cache allows you to optimize the performance of your Streamlit app.\n",
    "\n",
    "Streamlit provides a caching mechanism that allows your app to stay performant even when loading data from the web, manipulating large datasets, or performing expensive computations. This is done with the @st.cache decorator.\n",
    "\n",
    "When you mark a function with the @st.cache decorator, it tells Streamlit that whenever the function is called it needs to check a few things:\n",
    "\n",
    "The input parameters that you called the function with\n",
    "The value of any external variable used in the function\n",
    "The body of the function\n",
    "The body of any function used inside the cached function\n",
    "If this is the first time Streamlit has seen these four components with these exact values and in this exact combination and order, it runs the function and stores the result in a local cache. Then, next time the cached function is called, if none of these components changed, Streamlit will just skip executing the function altogether and, instead, return the output previously stored in the cache.\n",
    "\n",
    "The way Streamlit keeps track of changes in these components is through hashing. Think of the cache as an in-memory key-value store, where the key is a hash of all of the above and the value is the actual output object passed by reference.\n",
    "\n",
    "Finally, @st.cache supports arguments to configure the cache's behavior. You can find more information on those in our API reference.\n",
    "\n",
    "### How to use?\n",
    "You can simply add st.cache decorator on the preceding line of a custom function that you define in your Streamlit app. See the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "st.title('st.cache')\n",
    "\n",
    "# Using cache\n",
    "a0 = time()\n",
    "st.subheader('Using st.cache')\n",
    "\n",
    "@st.cache(suppress_st_warning=True)\n",
    "def load_data_a():\n",
    "  df = pd.DataFrame(\n",
    "    np.random.rand(2000000, 5),\n",
    "    columns=['a', 'b', 'c', 'd', 'e']\n",
    "  )\n",
    "  return df\n",
    "\n",
    "st.write(load_data_a())\n",
    "a1 = time()\n",
    "st.info(a1-a0)\n",
    "\n",
    "\n",
    "# Not using cache\n",
    "b0 = time()\n",
    "st.subheader('Not using st.cache')\n",
    "\n",
    "def load_data_b():\n",
    "  df = pd.DataFrame(\n",
    "    np.random.rand(2000000, 5),\n",
    "    columns=['a', 'b', 'c', 'd', 'e']\n",
    "  )\n",
    "  return df\n",
    "\n",
    "st.write(load_data_b())\n",
    "b1 = time()\n",
    "st.info(b1-b0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
